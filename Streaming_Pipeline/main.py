from pyspark.sql import SparkSession
import configs
import functions

# –õ–æ–≥—É–≤–∞–Ω–Ω—è
print(f"üîπ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ JAR-—Ñ–∞–π–ª: {configs.MYSQL_JAR_PATH}")
print(f"üîπ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MySQL: {configs.MYSQL_HOST}:{configs.MYSQL_PORT}/{configs.MYSQL_DATABASE}")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ Spark
spark = SparkSession.builder \
    .appName("AthleteDataPipeline") \
    .config("spark.jars", configs.MYSQL_JAR_PATH) \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4") \
    .getOrCreate()

# 1Ô∏è‚É£ –ß–∏—Ç–∞—î–º–æ `athlete_bio` –∑ MySQL
print("üì• –ß–∏—Ç–∞—î–º–æ `athlete_bio` –∑ MySQL...")
bio_df = functions.read_athlete_bio(spark)
print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {bio_df.count()} –∑–∞–ø–∏—Å—ñ–≤.")

# 2Ô∏è‚É£ –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –¥–∞–Ω—ñ
print("üîç –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –∑–∞–ø–∏—Å–∏ (height, weight)...")
filtered_bio_df = functions.filter_invalid_data(bio_df)
print(f"‚úÖ –ó–∞–ª–∏—à–∏–ª–æ—Å—å {filtered_bio_df.count()} –∑–∞–ø–∏—Å—ñ–≤ –ø—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó.")

# 3Ô∏è‚É£ –ß–∏—Ç–∞—î–º–æ `athlete_event_results` –∑ MySQL
print("üì• –ß–∏—Ç–∞—î–º–æ `athlete_event_results` –∑ MySQL...")
event_results_df = functions.read_athlete_event_results(spark)
print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {event_results_df.count()} –∑–∞–ø–∏—Å—ñ–≤.")

# 4Ô∏è‚É£ –ó–∞–ø–∏—Å—É—î–º–æ `athlete_event_results` —É Kafka
print(f"üì§ –ó–∞–ø–∏—Å—É—î–º–æ `athlete_event_results` —É Kafka (—Ç–æ–ø—ñ–∫ {configs.KAFKA_TOPIC_EVENTS})...")
functions.write_to_kafka(event_results_df, configs.KAFKA_TOPIC_EVENTS)

# 5Ô∏è‚É£ –ß–∏—Ç–∞—î–º–æ `athlete_event_results` –∑ Kafka
print(f"üì• –ß–∏—Ç–∞—î–º–æ `athlete_event_results` –∑ Kafka (—Ç–æ–ø—ñ–∫ {configs.KAFKA_TOPIC_EVENTS})...")
kafka_df = functions.read_from_kafka(spark, configs.KAFKA_TOPIC_EVENTS)

# ‚úÖ –î–æ–¥–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—î—é
print("üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–∏—Ä—ñ –¥–∞–Ω—ñ –∑ Kafka –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—î—é...")
kafka_df.show(5, truncate=False)

# 6Ô∏è‚É£ –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ JSON ‚Üí DataFrame
print("üîÑ –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ JSON —ñ–∑ Kafka —É DataFrame...")
event_df = functions.transform_kafka_data(kafka_df)

# ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ 3: –í–∏–≤—ñ–¥ 5 –∑–∞–ø–∏—Å—ñ–≤ –∑ `event_df`
print("üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ Kafka-—Ç–æ–ø—ñ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–¥–∞—î JSON —É Spark...")
event_df.show(5, truncate=False)

# 7Ô∏è‚É£ –û–±'—î–¥–Ω—É—î–º–æ `athlete_event_results` —ñ–∑ `athlete_bio`
print("üîó –û–±'—î–¥–Ω—É—î–º–æ `athlete_event_results` —ñ–∑ `athlete_bio` –∑–∞ `athlete_id`...")
joined_df = functions.join_data(event_df, filtered_bio_df)

# 8Ô∏è‚É£ –ê–≥—Ä–µ–≥—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
print("üìä –û–±—á–∏—Å–ª—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ–π –∑—Ä—ñ—Å—Ç —ñ –≤–∞–≥—É –ø–æ `sport`, `medal`, `sex`, `country_noc`...")
aggregated_df = functions.aggregate_data(joined_df)

# 9Ô∏è‚É£ –ó–∞–ø–∏—Å—É—î–º–æ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ —É Kafka
print(f"üì§ –ó–∞–ø–∏—Å—É—î–º–æ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ —É Kafka (—Ç–æ–ø—ñ–∫ {configs.KAFKA_TOPIC_AGGREGATED})...")
functions.write_to_kafka(aggregated_df, configs.KAFKA_TOPIC_AGGREGATED)

# üîü –ó–∞–ø–∏—Å—É—î–º–æ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ —É MySQL (`dag_palchyk1984.aggregated_athlete_results`)
print(f"üì• –ó–∞–ø–∏—Å—É—î–º–æ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ —É MySQL —Ç–∞–±–ª–∏—Ü—é {configs.MYSQL_AGG_TABLE}...")
functions.write_to_mysql(aggregated_df)

# –ó–∞–≤–µ—Ä—à—É—î–º–æ —Ä–æ–±–æ—Ç—É Spark
print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏.")
spark.stop()
